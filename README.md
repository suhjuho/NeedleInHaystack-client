<p align="center">
  <img width="400" alt="image" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/139360841/183ebf52-2a91-4e40-a381-c8ab5df22d7f">
</p>

<p align="center">
  Needle in haystackì€ IT í‚¤ì›Œë“œ ê²€ìƒ‰ì‹œ ìì²´ ì•Œê³ ë¦¬ì¦˜ ìˆœìœ„ì— ë”°ë¼ ì˜ìƒì„ ê²€ìƒ‰í•´ ì£¼ê³  ì˜ìƒ ì† ì½”ë“œ ì¶”ì¶œ ê¸°ëŠ¥ë„ ì œê³µí•˜ëŠ” ì˜ìƒ ê¸°ë°˜ì˜ ê²€ìƒ‰ì—”ì§„ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
</p>

<div align="center">

[Frontend repository](https://github.com/devsgk/NeedleInHaystack-client)
/
[Backend repository](https://github.com/devsgk/NeedleInHaystack-server)

</div>

## ğŸ“’ Contents
- [âœˆï¸ Demo](#ï¸-demo)
- [ğŸ”§ Tech Stacks](#-tech-stacks)
- [ğŸ™‹ğŸ»â€â™‚ï¸ Introduction](#ï¸-introduction)
- [ğŸ’ª Motivation](#-motivation)
- [ğŸ§ How does a "Search Engine" work?](#-how-does-a-search-engine-work)
- [â›°ï¸ Challenges](#ï¸-challenges)
  1. [Search Algorithm](#1-search-algorithm)
  2. [Spell check feature](#2-spell-check-feature)
  3. [Extract code from playing video](#3-extract-code-from-playing-video)
  4. [Automate Crawler](#4-automate-crawler)
- [ğŸ“š What I learned](#-what-i-learned)
- [â° Project timeline](#-project-timeline)

<br>

## âœˆï¸ Demo
To be updaed...

<br>

## ğŸ”§ Tech Stacks

### Client
![JavaScript](https://img.shields.io/badge/javascript-%23323330.svg?style=for-the-badge&logo=javascript&logoColor=%23F7DF1E)
![React](https://img.shields.io/badge/react-%2320232a.svg?style=for-the-badge&logo=react&logoColor=%2361DAFB)
![TailwindCSS](https://img.shields.io/badge/tailwindcss-%2338B2AC.svg?style=for-the-badge&logo=tailwind-css&logoColor=white)
![Vite](https://img.shields.io/badge/vite-%23646CFF.svg?style=for-the-badge&logo=vite&logoColor=white)

### Server
![NodeJS](https://img.shields.io/badge/node.js-6DA55F?style=for-the-badge&logo=node.js&logoColor=white)
![Express.js](https://img.shields.io/badge/express.js-%23404d59.svg?style=for-the-badge&logo=express&logoColor=%2361DAFB)
![MongoDB](https://img.shields.io/badge/MongoDB-%234ea94b.svg?style=for-the-badge&logo=mongodb&logoColor=white)

### Test
![Static Badge](https://img.shields.io/badge/vitest-8A2BE2?style=for-the-badge)
![Testing-Library](https://img.shields.io/badge/React%20Testing%20Library-%23E33332?style=for-the-badge&logo=testing-library&logoColor=white)
![JestDOM](https://img.shields.io/badge/Jest%20DOM-8A2BE2?style=for-the-badge)

### Deployment
![Netlify](https://img.shields.io/badge/netlify-%23000000.svg?style=for-the-badge&logo=netlify&logoColor=#00C7B7)
![AWS](https://img.shields.io/badge/Elastic%20Beanstalk-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)

<br>

## ğŸ™‹ğŸ»â€â™‚ï¸ Introduction

ë°”ë‹ë¼ ì½”ë”© ë¶€íŠ¸ìº í”„ì—ì„œ 4ì£¼ê°„ ì§„í–‰í•œ íŒ€ í”„ë¡œì íŠ¸ë¡œ, ì›¹ ê°œë°œ ë° ì½”ë”© ê´€ë ¨ ì˜ìƒì„ YouTubeì²˜ëŸ¼ ê²€ìƒ‰í•´ ì£¼ëŠ” ë™ì˜ìƒ ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„ ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.

ì €í¬ëŠ” ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ YouTubeì— API ìš”ì²­ì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë™ì˜ìƒ ë°ì´í„°ë¥¼ ì§ì ‘ í¬ë¡¤ë§í•˜ì—¬ ê³ ìœ í•œ ë°©ì‹ìœ¼ë¡œ ìì²´ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ë Œë”ë§í•˜ëŠ” ê²€ìƒ‰ ì—”ì§„ì„ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.

<br>

## ğŸ’ª Motivation
ê°œë°œ ê³µë¶€ë¥¼ í•˜ë©´ì„œ êµ¬ê¸€ë§ì€ ê±°ì˜ í•„ìˆ˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë„êµ¬ì˜€ìŠµë‹ˆë‹¤. íŠ¹íˆë‚˜ ìš”ì¦˜ì—ëŠ” ì˜ìƒ ì½˜í…ì¸ ì˜ í­ë°œì ì¸ ì¦ê°€ì™€ í•¨ê»˜ YouTubeì—ì„œ ì •ë³´ë¥¼ ì°¾ëŠ” ê²ƒì´ ìƒˆë¡œìš´ ê²€ìƒ‰ í‘œì¤€ì´ ë˜ì–´ ê°€ê³  ìˆìŒì„ ëŠë¼ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í™”ì— ì˜ê°ì„ ë°›ì•„, ê°œë°œ ê´€ë ¨ ì£¼ì œë¡œ í•œì •í•˜ì—¬ YouTubeì™€ ê°™ì€ íš¨ìœ¨ì ì¸ ì˜ìƒ ê²€ìƒ‰ ê²½í—˜ì„ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ê³  ì‹¶ë‹¤ëŠ”  ìƒê°ì„ í•˜ê²Œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì¼ìƒ ì†ì—ì„œ ë„ˆë¬´ë‚˜ë„ ë‹¹ì—°í•˜ê²Œ ì—¬ê²¨ì¡Œë˜ â€œê²€ìƒ‰â€ì— ëŒ€í•´ ì¡°ê¸ˆ ë” ê¹Šì´ ì•Œì•„ê°€ê³  ì‹¶ì€ ìš•êµ¬ê°€ ìƒê²¨ Needle in haystack í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

<br>

## ğŸ§ How does a "Search Engine" work?
Googleê³¼ ê°™ì€ ê²€ìƒ‰ ì—”ì§„ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë‹¤ìŒì˜ ì„¸ ë‹¨ê³„ë¡œ ì‘ë™í•©ë‹ˆë‹¤.

**1. í¬ë¡¤ë§**

: í¬ë¡¤ë§ì€ ê²€ìƒ‰ ì—”ì§„ì´ ìƒˆë¡œìš´ ì›¹ ì‚¬ì´íŠ¸ë‚˜ ë¬¸ì„œë¥¼ ì°¾ìœ¼ë ¤ê³  ì‹œë„í•˜ëŠ” ì´ˆê¸° ë‹¨ê³„ì…ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œëŠ” í¬ë¡¤ëŸ¬ ë˜ëŠ” ìŠ¤íŒŒì´ë”ë¼ê³  ì•Œë ¤ì§„ ìë™í™”ëœ ë´‡ì´ ì´ ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. ì´ë¯¸ ì•Œê³  ìˆëŠ” í˜¹ì€ ì£¼ì–´ì§„ ì›¹ í˜ì´ì§€ URLì—ì„œ ì‹œì‘í•˜ì—¬ í•´ë‹¹ í˜ì´ì§€ì— ì¡´ì¬í•˜ëŠ” ë§í¬ë“¤ì„ ë”°ë¼ê°„ ë‹¤ìŒ, ì—°ì†ì ì¸ ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒ í˜ì´ì§€ì— ìˆëŠ” ë§í¬ë¥¼ íƒ€ê³  ê°‘ë‹ˆë‹¤. ì´ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ í¬ë¡¤ëŸ¬ëŠ” ìƒˆë¡œìš´ ì½˜í…ì¸ ì™€ ê¸°ì¡´ ì½˜í…ì¸ ë¥¼ ì°¾ì•„ ê¸ì–´ ì˜¤ê²Œ ë©ë‹ˆë‹¤.

**2. ì¸ë±ì‹±**

: í¬ë¡¤ëŸ¬ê°€ ìŠ¤í¬ë©í•œ ìƒˆë¡œìš´ ì½˜í…ì¸ ëŠ” ì¸ë±ì‹±ì´ë¼ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë©ë‹ˆë‹¤. ì¸ë±ì‹±ì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ì»¨í…ì¸ ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê²Œ ë˜ëŠ”ë°, ì´ ë¶„ì„ì—ëŠ” ì½˜í…ì¸ ë¥¼ ì„¤ëª…í•˜ëŠ” í•µì‹¬ ë‹¨ì–´ì™€ ë¬¸êµ¬, ì½˜í…ì¸  ìœ í˜•ì„ ê¸°ë¡í•˜ëŠ” ê²ƒì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ì»¨í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•¨ìœ¼ë¡œì¨, ìœ ì €ë¡œë¶€í„° ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ë°œìƒë˜ë©´ ê°€ì¥ ê´€ë ¨ë„ê°€ ë†’ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë Œë”í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

**3. ê²€ìƒ‰ ì—”ì§„**

: ë§ˆì§€ë§‰ìœ¼ë¡œ ì—¬ëŸ¬ê°€ì§€ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ìœ ì €ì˜ ì¿¼ë¦¬ì™€ ê°€ì¥ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ë Œë”í•©ë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ì—ëŠ” TF-IDF, BM25, BM25F ë° PageRankì™€ ê°™ì€ ë‹¤ì–‘í•œ ìˆœìœ„ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê° ì½˜í…ì¸ ê°€ ì¿¼ë¦¬ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë°ì´í„°ë¥¼ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ìƒë‹¨ì— í‘œì‹œë˜ë„ë¡ í•©ë‹ˆë‹¤.

ì €í¬ëŠ” Googleì˜ Puppeteer ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ YouTube ë™ì˜ìƒ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ê¸ì–´ì˜¨ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê¸° ì „ì— ì¶”í›„ ì‚¬ìš©ìê°€ ê²€ìƒ‰ì‹œ ë¹ ë¥¸ ë°ì´í„° ê²€ìƒ‰ì„ ìœ„í•´ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì—­ì¸ë±ì‹±ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì‚¬ìš©ìì˜ ê²€ìƒ‰ ìš”ì²­ì„ ë°›ìœ¼ë©´ TF-IDF, BM25, BM25F, PageRank ë“± ë‹¤ì–‘í•œ ìˆœìœ„ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë°ì´í„°ë¥¼ ë¨¼ì € ë Œë”ë§í•˜ë„ë¡ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.

<br>

## â›°ï¸ Challenges
### 1. Search Algorithm

: Our first challenge was implementing a search algorithm. The role of the search algorithm is incredibly important in a search engine for finding and displaying the results a user wants.

We researched various search algorithms and attempted to implement them ourselves. First, we applied the TF-IDF algorithm. Using the TF-IDF algorithm allows us to calculate the importance of a word within a document.

<br>

**- TF (Term Frequency)**
<p>
  <img width="350" alt="TF" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/139360841/393b05ae-9a9b-44fb-af44-70a49f37d5ec">
</p>

: It is the number of times a particular word appears in a document, divided by the total number of words in that document. This shows the frequency of the word. The more frequently a word appears, the higher its value.

<br>

**- IDF (Inverse Document Frequency)**
<p>
  <img width="350" alt="IDF" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/139360841/f3880913-56ff-454b-9129-0f94543b99d1">
</p>

: It is the logarithm of the total number of documents divided by the number of documents that contain the word. As the number of documents containing the word increases, the IDF value decreases inversely. This represents the uniqueness, or the specific value, of the word.

<br>

**- TF-IDF**
<p>
  <img width="350" alt="TF-IDF" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/139360841/6ab61630-e75e-4fd7-9f3d-87c109edad3e">
</p>

: TF-IDF is simply the product of these two values, meaning that the higher the frequency of the word in a specific document and the fewer appearances in other documents, the higher its TF-IDF value.

1. Spell check feature
2. Extract code from playing video
3. Automate crawler

<br>

**- BM25 (Best Matching 25)**
<p>
  <img width="350" alt="BM25" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/139360841/7c55a664-0f31-4f31-ad19-6eeaaa6beed0">
</p>

: We utilized a modified version of the TF-IDF algorithm, known as the BM25 algorithm.

Though the formula might appear complex at first glance, it similarly employs the IDF and TF values I just mentioned. However, two constants, k1 and b, are added, which respectively ensure the maximum value of frequency and the minimum value of document length.

Moreover, the algorithm includes the calculation of the document's length and the average length of all documents to perform normalization based on document length.

In our project, we employed BM25F which basically calculates the BM25 algorithm values for each field, such as title, description, and script within a video, assigning different weights according to the relative importance of each field.

<br>

**- PageRank algorithm**
<p>
  <img width="350" alt="Ranking" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/139360841/25ab70e7-534a-46db-8d57-a0af951e26b3">
</p>

Next, we added the PageRank algorithm to our search algorithm suite.

While the previously mentioned methods focus on the importance of words, the PageRank algorithm, true to its name, denotes the importance of pages.

Web pages contain links that point to each other, and the PageRank value of a page can be determined using the PageRank values of other pages that link to it.

The calculated PageRank value represents the probability of reaching that web page, and a higher PageRank value can indicate a webpage that many people are likely to visit.

<br>

### 2. Spell check feature

: When using search engines, you might have noticed that they can automatically correct misspelled search queries. We wanted to implement such a spell-check feature.

Before developing the spell check functionality, we looked into how Google performs spell checking.

Google analyzes user patterns. For example, a user might search for something using a misspelled word and, not finding the desired information, re-search with the correct spelling.

By analyzing these recurring patterns among many users, Google can identify the relationship between the misspelled words and the correct terms. Then, when another user makes the same spelling mistake, Google automatically searches using the correct term.

However, in our case, we didnâ€™t have enough users and sufficient data to perform meaningful pattern analysis. Thus, we decided to adopt a different approach.

Our next challenge was this:

When using search engines, you might have noticed that they can automatically correct misspelled search queries. We wanted to implement such a spell-check feature.

Before developing the spell check functionality, we looked into how Google performs spell checking.

Google analyzes user patterns. For example, a user might search for something using a misspelled word and, not finding the desired information, re-search with the correct spelling.

By analyzing these recurring patterns among many users, Google can identify the relationship between the misspelled words and the correct terms. Then, when another user makes the same spelling mistake, Google automatically searches using the correct term.

However, we realized that our project faced a challenge due to a lack of users and insufficient data to perform meaningful pattern analysis. Thus, we decided to adopt a different approach.

We utilized the biGram and soundex algorithms.

Both algorithms function to check the similarity between two words.

- **Bigram and Soundex algorithm**

: It analyzes similarity by comparing the letters of words, while the Soundex algorithm compares the phonetics of words to assess similarity.

We recommended words with high similarity to the user's search query. To do this, we needed words to compare the search query against.

We collected these words during the crawling process. Every word was stored in the database during crawling, and when a user conducted a search, we found and suggested words with high similarity. However, we encountered two major issues in this process. 

1. Retrieving tens of thousands of words from the database took more time than expected.
2. Since the words were collected through crawling, some incorrect words were included.

To address these problems:

1. we improved accuracy by using an English word library filled with correct words.
2. We utilized a trie data structure to quickly determine if the user's search term was correct, applying the spelling check algorithm only in those cases.

These two methods helped us enhance both the speed and accuracy of our system.

<br>

### 3. Extract code from playing video
<p>
  <img width="350" alt="extract" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/139360841/19d22e2c-0c80-4819-9fd8-12d1252391b3">
</p>

: For this feature, we leveraged Optical Character Recognition (OCR) technology, which recognizes text in images, and we utilized Tesseract.js, the most widely used OCR engine in the Node.js environment. Initially, we thought extracting text from images would be challenging, but since the Tesseract OCR engine handles the extraction of text from images, we believed our task was simply to take screenshots of the playing video and let Tesseract recognize the text, which seemed feasible. Contrary to our expectations, we encountered more difficulties in taking screenshots than in extracting text.

Our first attempt was to directly access the video DOM element and then draw it on a canvas. The web allows direct access to DOM elements to capture their current state on a canvas. However, since we were embedding YouTube videos using an Iframe, we couldn't directly access the video DOM due to external resource access permission issues.

The solution we came up with was to use Puppeteer, as we did for crawling, to bypass and take screenshots directly from the actual YouTube webpage. Videos on the YouTube webpage are not in Iframe but in Video tags, making it possible to access the DOM. Utilizing this, we were able to take screenshots of the screen at the user's current video playback point based on two pieces of information: the video ID and the current playback time of the video the user was watching.

<br>

### 4. Automate Crawler
<p>
  <img width="350" alt="automate crawler" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/139360841/a529031c-6034-4c6a-bbb8-63439fcf2127">
</p>

- **First try**
: we hardcoded the entry URL when we wanted to crawl in **`crawl.js`** file, and manually execute this file from the terminal to start the crawling process. 

- **Second try**
For the second version, we implemented a admin page accessible only to administrators. This allowed us to input the entry URL via a text input field and control the crawler more dynamically through start and stop buttons on our website. 

- **Third try**

: To make the crawling process completely automatic, we deployed our crawler on AWS Lambda. By hosting the crawler on AWS Lambda, we could resolve the following two issues;

1. **Automation**: Unlike the previous versions that required manual on/off control, the last version of the crawler runs spontaneously at predetermined times. This allows us to focus on other development tasks without worrying about operating the crawler.
2. **Infinite Depth Crawling Problem**: The earlier crawler versions would run indefinitely until manually stopped, potentially straying off-topic from development-related videos to irrelevant content as time passed. Our service is designed to search for and watch development-related videos. By deploying the service on AWS Lambda, we adapted our strategy. Now, when a user navigates to a video watching page on our service, we store that video's ID. At specific times, the crawler accesses these stored video IDs and crawls only the top 5 related videos, avoiding the infinite depth issue. We also set a maximum function execution time to ensure the crawler stops automatically.

<br>

## ğŸ“š What I learned
Working on this project was gave us a huge opportunity to deeply understand the principles behind the search engines we use daily. Real search engines utilize machine learning, AI, and manage much larger data sets, so we see endless challenges and opportunities for improvement in our project moving forward.

Among the achievable aspects, one area we're considering is how to further enhance search accuracy to deliver more personalized results to users. Storing users' search histories in the database has been our practice, but leveraging additional information like location and age could offer even more tailored results.

As for search speed, as we anticipate managing increasingly larger datasets, we're contemplating how to maintain quick search responses. This involves strategizing on data management and exploring ways to ensure that search speeds remain fast despite the growing volume of information.

Continual improvement and adaptation to these challenges are our key focuses, aiming to provide a search engine that not only meets but exceeds user expectations in terms of accuracy, speed, and personalized experience.

<br>

## â° Project timeline
**2024.01.22 - 2024. 01.28**

- Brain storming for project ideas
- POC
- Planning
- KANBAN Task

**2024.01.29 - 2024.02.16**

- MERN stack environment setting
- Implement search engine
- Implement spell check feature
- Implement extract code feature
- Automate crawler

**2024.02.19 - 2024.02.25**

- README
- Deployment
