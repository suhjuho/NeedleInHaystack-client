<p align="center">
  <img width="400" alt="image" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/139360841/183ebf52-2a91-4e40-a381-c8ab5df22d7f">
</p>

<p align="center">
  Needle in Haystack은 IT 키워드 검색 시 자체 알고리즘 순위에 따라 영상을 검색해 주고 영상 속 코드 추출 기능도 제공하는 영상 기반의 검색엔진 서비스입니다.
</p>

<div align="center">

[Deployed](https://web.nih.world)
|
[Frontend Repository](https://github.com/suhjuho/NeedleInHaystack-client)
|
[Backend Repository](https://github.com/suhjuho/NeedleInHaystack-server)
|
[Flutter App Repository](https://github.com/Team-Office360/NeedleInHaystack-flutter)

</div>

<br />

<p align="center">
  <img width="600px" alt="image" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/ae342b09-c038-4e20-ab7a-09f30d5fea6c">
</p>

<br />

## 🗂️ 목차

- [🙋🏻‍♂️ 프로젝트 소개](#️-프로젝트-소개)
- [💪 프로젝트 동기](#-프로젝트-동기)
- [🔧 기술 스택](#-기술-스택)
- [🧐 검색 엔진에는 어떤 과정들이 있을까?](#-검색-엔진에는-어떤-과정들이-있을까)
  - [1. '크롤링'을 통한 데이터 수집](#1-크롤링-crawling)
  - [2. '인덱싱'을 활용하여 데이터 저장](#2-인덱싱-indexing)
  - [3. 순위 도출을 위한 '검색 엔진'](#3-검색-엔진-search-engine)
- [⛰️ 챌린지](#️-챌린지)
  - [1. 어떻게 검색을 위한 유튜브 영상 데이터를 수집할 수 있을까?](#️-1-어떻게-검색을-위한-유튜브-영상-데이터를-수집할-수-있을까)
    - [1.1 상황: Youtube API을 사용하지 않고 데이터를 수집해야한다](#11-상황-youtube-api을-사용하지-않고-데이터를-수집해야한다)
    - [1.2 아이디어: Puppeteer 라이브러리를 활용하자](#12-아이디어-puppeteer-라이브러리를-활용하자)
    - [1.3 구현 과정: 자동화된 크롤러를 구현](#13-구현-과정-크롤링-자동화)
    - [1.4 결과: 크롤링 소스코드를 AWS Lambda로 이동](#14-결과-크롤링-소스코드를-aws-lambda로-이동)
  - [2. 어떻게 사용자가 원하는 검색 결과를 보여줄 수 있을까?](#️-2-어떻게-사용자가-원하는-검색-결과를-보여줄-수-있을까)
    - [2.1 랭킹 알고리즘을 활용하여 순위 도출하기](#21-tf-idf-알고리즘을-활용하여-순위도출)
  - [3. 랭킹 알고리즘을 실행해야 하는 시점은 언제일까?](#-3-랭킹-알고리즘을-실행해야-하는-시점은-언제일까)
    - [3.1 역색인(Inverted Index) 작업에 랭킹 알고리즘(Ranking Algorithm) 추가하기](#31-역색인inverted-index-작업에-랭킹-알고리즘ranking-algorithm-추가하기)
- [👀 사용자 경험 (User Experience)](#-사용자-경험-user-experience)
  - [1. 많은 검색 결과들을 어떻게 화면에 렌더링해줘야 할까?](#1-많은-검색-결과들을-어떻게-화면에-렌더링해줘야-할까)
    - [1.1 useInfiniteQuery로 무한 스크롤 구현하기](#1-1-useinfinitequery로-무한-스크롤-구현하기)
  - [2. 검색어에 오타가 있다면 어떤 식으로 대응해줘야 할까](#2-검색어에-오타가-있다면-어떤식으로-대응해줘야-할까)
    - [2.1 상황: 구글 같은 검색엔진에서는 어떻게 스펠링 체크를 해줄까?](#2-1-상황-구글-같은-검색엔진에서는-어떻게-스펠링-체크를-해줄까)
    - [2.2 아이디어: 틀린 검색어와 비슷한 단어 찾아주기](#2-2-아이디어-틀린-검색어와-비슷한-단어-찾아주기)
    - [2.3 구현: 영단어 라이브러리와 트라이 자료구조로 올바른 검색어 추천](#2-3-구현-영단어-라이브러리와-트라이-자료구조로-올바른-검색어-추천)
- [🕹️ 추가 기능 (Advanced Features)](#️-advanced-features)
  - [1. 영상 속 코드를 타이핑하지 않고 바로 가져다 쓸 수 없을까?](#1-영상-속에-코드를-타이핑하지-않고-바로-가져다-쓸-수-없을까)
    - [1.1 문제: 유튜브 영상은 스크린 캡쳐를 할 수 없다](#1-1-유튜브-영상은-스크린-캡쳐를-할-수-없다)
    - [1.2 Headless Browser를 이용하자](#1-2-headless-browser를-이용하자)
    - [1.3 드래그 앤 드롭으로 원하는 코드만 추출 하기](#1-3-드래그-앤-드롭으로-원하는-코드만-추출-하기)
    - [1.4 구현: 추출한 코드를 코드 에디터로 편집](#1-4-구현-추출한-코드를-코드-에디터로-편집)
    <!-- - [2. 앱 애플리케이션 개발로 사용자 접근성 확장하기](#2-앱-애플리케이션-개발로-사용자-접근성-확장하기)
    - [2.1 Flutter 앱의 webview를 활용하여 구현]() -->
- [⏰ 프로젝트 타임라인](#-프로젝트-타임라인)
- [📚 프로젝트 소감](#-프로젝트-소감)

<br />

# 🙋🏻‍♂️ 프로젝트 소개

4주간 진행한 팀 프로젝트로, 웹 개발 및 코딩 관련 영상을 YouTube와 같이 검색해 주는 동영상 기반 검색 엔진 서비스입니다.

저희는 단순히 YouTube에 API 요청을 보내 데이터를 가져오는 것이 아니라, 동영상 데이터를 직접 크롤링하여 고유한 방식으로 자체 데이터베이스에 저장하였습니다. 그리고 수집한 데이터를 기반으로 사용자의 검색어와 가장 관련있는 비디오 데이터들을 렌더링하는 검색 엔진을 구현하였습니다.

<br />

# 💪 프로젝트 동기

개발 공부를 하면서 구글링은 거의 필수적으로 사용하는 도구였습니다. 특히나 요즘에는 영상 콘텐츠의 폭발적인 증가와 함께 YouTube에서 정보를 찾는 것이 새로운 검색 표준이 되어 가고 있음을 느끼게 되었습니다.

이러한 변화에 영감을 받아, 개발 관련 주제로 한정하여 YouTube와 같은 효율적인 영상 검색 경험을 제공하는 서비스를 개발하고 싶다는 생각을 하게 되었습니다. 그리고 일상 속에서 너무나도 당연하게 여겨졌던 “검색”에 대해 조금 더 깊이 알아가고 싶은 욕구가 생겨 프로젝트를 시작하게 되었습니다.

<br />

# 🔧 기술 스택

### 클라이언트

![JavaScript](https://img.shields.io/badge/javascript-%23323330.svg?style=for-the-badge&logo=javascript&logoColor=%23F7DF1E)
![React](https://img.shields.io/badge/react-%2320232a.svg?style=for-the-badge&logo=react&logoColor=%2361DAFB)
![TailwindCSS](https://img.shields.io/badge/tailwindcss-%2338B2AC.svg?style=for-the-badge&logo=tailwind-css&logoColor=white)
![Vite](https://img.shields.io/badge/vite-%23646CFF.svg?style=for-the-badge&logo=vite&logoColor=white)

### 서버

![NodeJS](https://img.shields.io/badge/node.js-5FA04E?style=for-the-badge&logo=node.js&logoColor=white)
![Express.js](https://img.shields.io/badge/express.js-%23404d59.svg?style=for-the-badge&logo=express&logoColor=%2361DAFB)
![MongoDB](https://img.shields.io/badge/MongoDB-47A248.svg?style=for-the-badge&logo=mongodb&logoColor=white)
![Mongoose](https://img.shields.io/badge/Mongoose-880000.svg?style=for-the-badge&logo=mongoose&logoColor=white)
![Puppeteer](https://img.shields.io/badge/puppeteer-%40B5A4.svg?style=for-the-badge&logo=puppeteer&logoColor=white)
![Tesserect.js](https://img.shields.io/badge/Tesserect.js-%23646C0F.svg?style=for-the-badge&logo=tesserect&logoColor=white)

### 테스트

![Static Badge](https://img.shields.io/badge/vitest-8A2BE2?style=for-the-badge)
![Testing-Library](https://img.shields.io/badge/React%20Testing%20Library-%23E33332?style=for-the-badge&logo=testing-library&logoColor=white)
![JestDOM](https://img.shields.io/badge/Jest%20DOM-8A2BE2?style=for-the-badge)

### 배포

![Netlify](https://img.shields.io/badge/netlify-%23000000.svg?style=for-the-badge&logo=netlify&logoColor=#00C7B7)
![AWS](https://img.shields.io/badge/Elastic%20Beanstalk-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)

<br />

# 🧐 검색 엔진에는 어떤 과정들이 있을까?

Google과 같은 검색 엔진은 기본적으로 다음의 세 단계로 작동합니다.

## 1. 크롤링 (Crawling)

크롤링은 검색 엔진이 새로운 웹 사이트나 문서를 찾으려고 시도하는 초기 단계입니다. <br /> 이 단계에서는 크롤러 또는 스파이더라고 알려진 자동화된 봇이 이 작업을 실행합니다. 이미 알고 있는 혹은 주어진 웹 페이지 URL에서 시작하여 해당 페이지에 존재하는 링크들을 따라간 다음, 연속적인 방식으로 다음 페이지에 있는 링크를 타고 갑니다. 이 프로세스를 통해 **크롤러는 새로운 콘텐츠와 기존 콘텐츠를 찾아 긁어 오게 됩니다.**

## 2. 인덱싱 (Indexing)

크롤러가 스크랩한 새로운 콘텐츠는 **인덱싱**이라는 프로세스를 통해 데이터베이스에 저장됩니다. <br /> 인덱싱을 생성하는 동안 콘텐츠의 내용을 분석하게 되는데, 이 분석에는 콘텐츠를 설명하는 핵심 단어와 문구, 콘텐츠 유형을 기록하는 것이 포함될 수 있습니다. 이렇게 콘텐츠를 분석하여 데이터베이스에 저장함으로써, 사용자로부터 검색 쿼리가 발생하면 가장 관련도가 높은 검색 결과를 렌더할 수 있게 됩니다.

## 3. 검색 엔진 (Search Engine)

마지막으로 여러 가지 검색 알고리즘을 통해 사용자의 쿼리와 가장 관련된 데이터를 반환합니다. <br /> 순위 도출을 위해 TF-IDF, BM25, BM25F 및 PageRank와 같은 다양한 순위 알고리즘을 사용하여 각 콘텐츠가 쿼리와 얼마나 관련성이 있는지 평가하게 됩니다. 이를 통해 가장 관련성이 높은 데이터를 검색 결과 페이지 상단에 표시되도록 합니다.

<!-- 저희는 Google의 Puppeteer 라이브러리를 사용하여 YouTube 동영상 데이터를 크롤링하였습니다. 그리고 긁어온 정보를 데이터베이스에 저장하기 전에 추후 사용자가 검색 시 빠른 데이터 검색을 위해 데이터 전처리 및 역색인을 수행하였습니다. 그리고 사용자의 검색 요청을 받으면 TF-IDF, BM25, BM25F, PageRank 등 다양한 순위 알고리즘을 사용하여 가장 관련성이 높은 데이터를 먼저 렌더링하도록 구현하였습니다. -->

<br />

# ⛰️ 챌린지

## 🕷️ 1. 어떻게 검색을 위한 유튜브 영상 데이터를 수집할 수 있을까?

검색 엔진을 개발하기 위해서는 검색에 사용될 데이터들을 수집해줘야 합니다. <br />저희 프로젝트는 검색 대상은 유튜브 영상 중 웹 개발과 코딩 관련 영상들로 한정하였습니다. 즉, 먼저 유튜브 영상의 데이터를 수집하기 위한 방법을 고민해야 했습니다.

### 1.1 상황: Youtube API을 사용하지 않고 데이터를 수집해야한다

Youtube API를 활용하면 해당 유튜브 영상의 정보를 수집할 수 있습니다. <br /> 이는 무료로 사용할 수 있지만 **사용량에 제한**이 있다는 단점이 있습니다. 많은 양의 데이터를 수집하기 위해서는 Youtube API를 사용하는 것은 한계가 있었습니다. 저희는 이를 해결하기 위해 자체적으로 크롤러를 제작하기로 했습니다.

[크롤링](#1-크롤링-crawling)은 검색 엔진이 새로운 웹 사이트나 문서를 찾으려고 시도하는 초기 단계입니다. <br />이 단계에서는 크롤러 또는 스파이더라고 알려진 자동화된 봇이 웹 페이지 URL에 접속하고 페이지에 연결된 링크를 타고 새로운 페이지로 이동합니다. 이 프로세스를 통해 크롤러는 새로운 콘텐츠와 기존 콘텐츠를 찾아 긁어 오게 됩니다.
저희는 Youtube API를 사용하지 않고 영상 데이터들을 수집하기 위해 이 크롤러를 제작했습니다.

### 1.2 아이디어: Puppeteer 라이브러리를 활용하자

크롤링을 하기 위해서는 URL을 이용해 해당 페이지에 접근하고 정보를 가져와야 합니다. <br /> 저희는 Node.js 환경에서 크롤링을 구현하기 위해 웹 스크래핑에 자주 사용되는 Cheerio 라이브러리를 사용해습니다. <br />이때 문제가 발생했습니다. 유튜브 웹 페이지는 동적으로 생성되는 콘텐츠가 많아 HTML 및 XML을 파싱하고 조작하는 Cheerio라이브러리로는 스크래핑하기에 부족한 점이 많았습니다.

저희는 Google의 **Puppeteer 라이브러리**로 변경하여 YouTube 영상 데이터를 크롤링하였습니다.

Puppeteer 라이브러리를 사용하면
**Headless Browser Control 기능을 통해 브라우저를 직접 조작할 수 있습니다.** puppeteer는 웹 페이지의 이미지와 동적으로 생성되는 콘텐츠를 쉽게 다룰 수 있습니다. 또한 버튼 클릭과 같은 사용자 인터랙션을 시뮬레이션하여 영상의 설명 글 혹은 자막등을 스크래핑하는데 도움이 되었습니다.

> Headless Browser란? GUI(Graphical User Interface)가 없는 웹 브라우저로 일반적이 웹 브라우저와 유사한 환경에서 웹페이지를 자동으로 제어할 수 있지만 명령줄 인터페이스 또는 네트워크 통신을 통해 실행됩니다.

저희가 제작한 크롤러는 유튜브 영상 데이터(title, description, keyword, transcript, youtubeId 등)를 수집합니다.

### 1.3 구현 과정: 크롤링 자동화

저희는 총 3가지 단계를 거치면서 크롤러를 제작했습니다.

<p align="center">
  <img width="500px" alt="automate crawler" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/efc89e6b-c237-4031-8e79-7c9ccc67b15a">
</p>

### 1.3.1. 터미널에서 수동 실행

첫번째는 **터미널에서 수동**으로 실행하는 방법입니다. 서버 내에서 crawler.js라는 파일을 만들어 두고, entry URL 값을 하드 코딩으로 적어둔 뒤에 직접 해당 파일을 터미널에서 실행시켜서 돌아가게 하는 방식으로 진행했습니다.

- 단점: 터미널에서 직접 실행시키는 방법은 서비스 측면에서 효율적이지 못하다는 점과 매번 entry URL을 수정하며 실행해야 한다는 점이었습니다.

### 1.3.2. Admin 페이지로 크롤러 관리

<p align="center">
  <img width="500px" alt="automate crawler" src="https://github.com/suhjuho/nemonemo/assets/133403759/4abd154a-34b1-4edd-b5a1-e6321484b113">
</p>

두번째 방식은 **admin 페이지에서 UI**를 통해 크롤러 동작시키는 방법입니다. <br /> 관리자만 접근 가능한 admin 페이지를 만들어 input 창에 엔트리 URL을 입력하고 시작과 중지 버튼으로 웹 브라우저에서 직관적인 조작을 가능하게 만들었습니다. 또한 SSE(Server Sent Event) 기능을 활용하여 크롤링 동작 시 각각의 크롤링의 성공 및 실패 결과를 확인할 수 있도록 하였습니다.

크롤러가 동작하면서 해당 동영상 데이터를 가져와 db에 저장하거나, 크롤링시 이미 db에 있는 중복된 영상이라던가, 데이터가 충분하지 않은 경우는 db에 저장하지 못했다는 메시지를 띄어줍니다. 하지만 여전히 자동화가 되지 않는 문제가 있었습니다.

- 장점: 원하는 시점에 엔트리 URL을 입력하여 크롤링을 실행할 수 있습니다.
- 단점: 자동으로 실행되지 않기 때문에 매번 수동으로 실행해주어야 합니다.

### 1.4 결과: 크롤링 소스코드를 AWS Lambda로 이동

최종적으로는 **AWS Lambda**를 활용하였습니다.
사용자가 비디오를 시청하면 해당 비디오들의 id들을 데이터베이스에 저장합니다. <br />일정 시간마다 작동하는 크롤러(crawler)는 DB에 쌓인 비디오 id들을 차례로 돌면서 해당 비디오의 연관 동영상 중 상위 5개의 영상을 크롤링해오고 해당 비디오 id는 DB에서 제거됩니다. 크롤러(crawler)는 데이터베이스에 저장된 비디오들의 id 값들이 없거나 미리 설정된 함수 작동시간이 지나면 자동으로 종료되도록 했습니다.

1. 첫번째는 **자동화**입니다. <br />AWS EventBridge의 cron 표현식을 사용하여 반복 작업을 예약하여 일정 시간마다 크롤러를 작동시키도록 설정했습니다. 저희 서비스 사용자들의 일간 영상 시청 횟수를 파악한 후 현재 크롤러 실행 주기는 24시간으로 설정했습니다.

2. 두번째는 **무한 깊이 탐색** 문제입니다. <br />이전의 방법들은 크롤러(crawler)를 한번 실행하게 되면, 수동으로 종료할 때 까지 무한하게 돌아가는 로직이었습니다. <br /> 저희 서비스는 개발 관련 영상에 대해서 검색하고 시청할 수 있는 서비스입니다. 하지만 크롤러가 제한 없이 동작하게 되면 어느 시점부터는 개발과 관련 없는 영상들까지 접근하는 문제가 생겼습니다. <br />이를 해결하기 위해 한 비디오의 url에 접근하면 연관 동영상 중 상위 5개의 영상을 크롤링해오고 다음 비디오로 넘어가도록 했습니다.

<br />

## 🖥️ 2. 어떻게 사용자가 원하는 검색 결과를 보여줄 수 있을까?

검색 엔진에서는 사용자가 입력한 검색어와 가장 관련도가 높은 결과를 상단에 보여주어야 합니다. 이를 위해 검색어마다 영상의 관련도 순위를 정해주어야합니다.

### 2.1 TF-IDF 알고리즘을 활용하여 순위도출

순위 도출을 위해 다양한 랭킹 알고리즘을 활용할 수 있습니다. <br /> 저희는 순위 도출을 위해 다양한 알고리즘을 조사해보았습니다. 그 중 저희가 첫 번째로 적용한 알고리즘은 정보 검색 및 텍스트 마이닝에서 가장 널리 사용되는 **TF-IDF 알고리즘**입니다.

**TF-IDF 알고리즘**을 사용하면 콘텐츠 안에서 핵심 단어의 중요도를 구할 수 있습니다. <br />
**TF는 Term Frequency**로 한 콘텐츠에서 해당 단어가 등장한 횟수를 모든 단어가 등장한 횟수로 나눈 값입니다. 즉, 단어의 빈도수를 나타냅니다. 단어가 많이 등장할수록 그 값이 증가하게 됩니다.<br/>
**IDF는 Inverse Document Frequency**로 총 콘텐츠의 개수를 단어가 등장하는 콘텐츠의 수로 나눈 값입니다. 해당 단어가 등장하는 콘텐츠의 수가 많아질수록 반비례하여 작아집니다. 즉 단어의 고유값을 나타냅니다.

- 장점: 간단하고 효율적으로 문서의 주요 키워드를 사용해 점수를 계산할 수 있습니다.
- 단점: 문서의 길이에 따라 가중치가 왜곡 될 수 있습니다.

<p align="center">
  <img width="500px" alt="tf-idf" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/b58c2908-5484-47c4-a8df-d3127d533c1a">
</p>

**TF-IDF**는 TF와 IDF의 곱으로 계산됩니다. 즉, 콘텐츠에 해당 단어가 많이 등장하고, 다른 콘텐츠에는 등장하지 않을 수록 그 값이 높아집니다.

### 2.2 BM25알고리즘을 활용하여 순위 도출

저희는 TF-IDF를 조금 변형하여 알고리즘인 BM25 알고리즘으로 변경하였습니다.
두 알고리즘을 비교해보면 BM25 알고리즘은 좀 더 복잡한 가중치 계산을 사용하여 문서안에 단어의 등장 빈도뿐만 아니라 문서 길이, 단어와 문서간의 일치도를 고려하여 더 정교하게 값을 계산합니다.

- **문서의 길이**를 고려하여 가중치를 계산하여 문서의 길이가 긴 경우에는 단어의 빈도에 더 낮은 가중치를 부여합니다.
- **검색어와 문서간의 일치도**를 고려하여 가중치를 계산합니다. 검색어와 문서에 공통으로 등장하는 단어의 수를 고려하여 가중치를 계산합니다.

BM25 알고리즘에는 상수 k1, b가 추가됩니다. 두 상수값은 빈도수의 최댓값, 문서 길이의 최솟값을 보장해 주는 역할을 합니다.

<p align="center">
  <img width="500px" alt="bm25" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/79dcd620-e2a0-4db8-8622-c825caf5aa38">
</p>

현재는 콘텐츠의 필드별로 값을 계산하는 방식인 BM25F 알고리즘을 적용하였습니다, 비디오에 있는 다양한 필드인 `title`, `description`, `script`, `tag`에 부여된 상대적인 중요도에 따라 무게를 다르게 주었습니다. title같이 상대적으로 더 중요하다고 생각되는 필드에 더 큰 무게를 주어 계산하였습니다.

- 장점: 문서의 길이를 보정할 수 있습니다. BM25F를 사용하면 필드별로 다른 가중치를 줄 수 있습니다.
- 단점: 파라미터 값을 조정해줘야 하고, 계산이 상대적으로 복잡합니다.

### 2.3 PageRank 알고리즘을 활용하여 웹 페이지의 중요도 구하기

다음으로 순위 도출에 더 다양한 요소를 추가하기 위해 PageRank 알고리즘을 검색 알고리즘에 추가했습니다.

<p align="center">
  <img width="500px" alt="bm25_알고리즘" src="https://github.com/suhjuho/NeedleInHaystack-client/assets/133403759/f2700d46-82e8-479d-9ac9-ee6b40c6e818">
</p>

PageRank는 웹 페이지의 중요도를 나타내는 대표적인 알고리즘입니다. 지금까지 사용한 BM25 알고리즘은 검색어안의 단어와 문서간의 연관도를 계산했다면 PageRank는 검색어와는 상관없이 해당 **페이지의 중요도**를 구하게 됩니다. 페이지의 중요도는 단순히 말하면 사용자들이 많이 찾는 페이지를 의미합니다.

PageRank 알고리즘은 얼마나 많은 사용자들이 해당 웹 페이지를 찾을지를 확률적으로 구하여 페이지의 중요도를 계산합니다. <br />웹 페이지들은 서로를 가리키는 링크들을 가지고 있습니다. 해당 페이지를 가리키는 다른 페이지들의 PageRank값을 이용하여 현재 페이지의 PageRank값을 구할 수 있습니다. 이렇게 구한 PageRank 값은 해당 웹 페이지로 도달할 확률을 나타내고, 페이지랭크 값이 클 수록 많은 사람이 찾는 웹페이지라고 생각할 수 있습니다.

결국 BM25F알고리즘과 PageRank알고리즘을 사용해 검색어와 연관도가 높고 사람들이 많이 찾는 검색결과를 반환할 수 있었습니다.

<br />

## 🤨 3. 랭킹 알고리즘을 실행해야 하는 시점은 언제일까?

초기에는 사용자가 검색 시에 해당 키워드에 해당하는 비디오들에 대해서 랭킹 알고리즘을 이용해 점수를 구해주고 순위대로 결과를 보여주었습니다. 하지만 검색마다 점수를 구하는 로직은 시간이 많이 소요된다는 치명적인 단점이 있었습니다.

이를 해결하기 위해서 **랭킹 알고리즘을 실행하는 시점**을 변경해 줄 필요가 있었습니다.

### 3.1 역색인(Inverted Index) 작업에 랭킹 알고리즘(Ranking Algorithm) 추가하기

수집한 데이터들(영상 데이터)은 키워드(영상에 포함된 단어)들은 기준으로 검색하기 위하여 역색인(inverted index) 작업을 거친 후에 DB에 저장됩니다. 해당 키워드를 가진 비디오들은 배열 형태로 저장되고 랭킹 알고리즘을 통해 구해진 점수(키워드와의 연관도)를 가집니다.

> 아래 코드는 크롤링해온 영상 데이터를 역색인 작업을 거쳐 저장한 데이터베이스의 `keyword` 스키마 입니다.

```js
const keywordSchema = new mongoose.Schema({
  text: {
    type: String,
    required: true,
    unique: true,
  },
  videos: {
    type: [
      {
        videoId: {
          type: Schema.Types.ObjectId,
          ref: "Video",
        },
      },
    ],
    default: [],
    required: true,
  },
});
```

영상과 관련된 키워드를 뽑아 키워드는 `text` 필드에 저장하고 관련 영상들을 `videos` 필드에 배열 형태로 저장합니다. 비디오 데이터 속에 들어있는 키워드들을 뽑아 키(key) 로 정하고 비디오를 값(value)으로 사용하는 작업으로 역색인이라고 부릅니다. 역색인을 하여 저장하는 이유는 검색시 해당 키워드에 관련된 데이터(영상 데이터)들을 빠르게 탐색하고 가져오기 위해서입니다.

<br />

크롤링과 동시에 해당 비디오와 키워드간의 연관도 점수를 구하고 비디오들이 담긴 배열은 연관도를 기준으로 내림차순을 정렬하여 저장하였습니다. <br /> 미리 DB에 키워드 별로 동영상들과의 연관도 점수가 구해지고 미리 정렬되어 있기 때문에 사용자가 키워드를 검색했을 시 빠른 속도로 결과를 보여줄 수 있게 되었습니다.

결국 검색 속도를 데이터 500개의 영상을 크롤링한 상태에서 테스트한 결과 **검색 속도를 7초에서 0.5초 이하로 줄이며 성능 향상**을 이루어 냈습니다.

<p align="center">
  <img width="500px" alt="search-before" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/06950a9a-9110-4112-a311-7509d8139cfe">
  <img width="500px" alt="search-after" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/86b4d14c-2fb3-4139-a801-8af91125f6a7">
</p>

<br />

# 👀 사용자 경험 (User Experience)

## 1. 많은 검색 결과들을 어떻게 화면에 렌더링해줘야 할까?

구글의 검색결과는 몇십만개의 결과가 나옵니다. 그 모든 결과를 한번에 화면에 렌더링한다면 분명 성능에 문제가 생길 것 입니다.<br />
저희는 어떤 방법으로 이를 해결할까 고민해보았습니다. 구글의 경우는 무한스크롤과 Load More버튼을 활용하고 있었습니다. <br />
저희는 끊김없이 검색 결과를 보여줘 사용자 경험 측면에서 좋고 모바일 환경에서도 사용하기 좋은 무한스크롤 방식을 사용하기 했습니다.

### 1-1. useInfiniteQuery로 무한 스크롤 구현하기

<p align="center">
  <img width="500px" alt="image" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/929fa80d-10d1-40a8-b24d-5deadd4e293f">
</p>

TanStack query 라이브러리의 `useInfiniteQuery` 훅을 활용하여 무한 스크롤을 구현하였습니다. Tanstack query를 활용하면 다음과 같은 장점을 가질 수 있습니다.

1. 자동 캐싱 및 상태 관리

- 데이터를 자동으로 캐싱하고, 데이터 패칭 상태를 관리해줍니다. 이를 이용해 로딩 상태를 표시해주었고, 데이터 페칭 작업의 성능도 향상시킬 수 있었습니다.

2. 자동 페이징 및 데이터 병합

- 페이징을 자동으로 처리해주어 한번에 페치할 데이터의 양을 미리 지정해주면 추가 데이터를 페칭할때 기존 데이터와 병합하는 로직을 간단하게 구현할 수 있어 더 쉽게 관리해줄 수 있었습니다.

### 에러 핸들링 적용하여 시스템의 안정성을 확보하고 사용자 경험 향상

검색시에는 예상치 못한 에러가 발생할 수 있습니다. 오타로 인해 검색 결과가 없는 경우도 있고, 사용자가 검색한 결과에 대해 관련있는 결과가 없는 경우도 있습니다. <br />
이런 예외 케이스들에 대해 에러 핸들링 적용하여 사용자에게 수정된 검색어를 보여주거나 빈 화면 대신 검색결과가 없다는 메세지를 대신 보여줍니다. <br />이를 통해 시스템의 안정성을 확보하고 사용자 경험 향상하였습니다.

<br />

## 2. 검색어에 오타가 있다면 어떤식으로 대응해줘야 할까?

<p align="center">
  <img width="500px" alt="image" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/5b4d8e31-3ad1-4a69-b8a1-26e62c24a5b2" />
</p>

### 2-1 상황: 구글 같은 검색엔진에서는 어떻게 스펠링 체크를 해줄까?

검색 엔진을 사용할 때 철자가 틀린 검색어가 자동으로 수정되는 되는 기능은 보편적으로 적용되고 있습니다. 저희는 이러한 맞춤법 검사 기능을 구현하기로 했습니다. 맞춤법 검사 기능을 개발하기 전에 Google이 맞춤법 검사를 수행하는 방식을 살펴봤습니다.

Google은 **사용자 패턴**을 분석합니다. <br /> 예를 들어 사용자가 철자가 틀린 단어인 `javascrpt`를 입력하여 무언가를 검색한 후 원하는 정보를 찾지 못하면 올바른 철자인 `javascript`로 다시 검색을 진행하게 됩니다. 수정한 검색어로 원하는 정보를 찾은 사용자는 더이상 검색을 하지 않습니다.

이러한 패턴을 분석하여 오타가 있는 단어 `javascrpt`와 올바른 단어 `javascript` 간의 연관성이 있다고 판단하게 되고, 이후 같은 패턴을 보이는 사용자에게 올바른 단어로 대신 검색을 하거나 검색을 수정하도록 제안해줍니다.

<p align="center">
  <img width="600px" alt="google-search-pattern" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/90fd11bd-1a27-40af-9a26-f3e05ef438ef" />
</p>

Google은 많은 사용자들의 이러한 반복적인 패턴을 분석하여 철자가 틀린 단어와 올바른 용어 사이의 관계를 파악할 수 있습니다. 그런 다음 다른 사용자가 동일한 철자 오류를 범하면 Google은 자동으로 올바른 용어를 사용하여 검색합니다.

하지만 저희의 경우 의미 있는 패턴 분석을 수행하기에 충분한 사용자 수와 데이터가 없었습니다. 따라서 우리는 다른 접근 방식을 채택하기로 결정했습니다.

### 2-2. 아이디어: 틀린 검색어와 비슷한 단어 찾아주기

저희는 `Bigram`과 `Soundex` 알고리즘을 활용하여 스펠링 체크 기능을 구현하였습니다.두 알고리즘 모두 두 단어의 유사도를 확인하는 기능을 합니다.

1. `Bigram` 알고리즘은 단어의 글자를 비교하여 유사도를 분석합니다. 인접한 두 글자 쌍을 고려하여 문맥을 이해하고 단어들 간의 유사도를 측정합니다.

   예시와 함께 이해를 해보면, 사용자가 오타가 있는 단어인 "javascrpt"를 입력하였다면 아래와 같이 나누어지고 올바른 단어인 "javscript"는 아래예시 처럼 나누어집니다.

    <p align="center">
      <img width="600px" alt="google-search-pattern" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/c4fa7db4-1cbc-4fb3-a2a2-92d40864149b" />
    </p>

   이렇게 나뉜 단어에 대하여 얼마나 같은 글자들이 공유되어있는지 계산을 합니다.

   전체 경우인 합집합 [ja av va as sc cr rp, ri ip, pt]과 공통으로 가지는 교집합 [ja av va as sc cr pt]의 길이를 구하여 전체 경우에서 얼마만큼 공통으로 가지는지 구합니다.

    <p align="center">
      <img width="600px" alt="google-search-pattern" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/b9b4f033-737e-4eab-b6de-ce9eaac456d0" />
    </p>

   교집합의 길이 / 합집합의 길이 = 7 / 10. <br /> 즉, 0.7의 유사도를 가진다고 판단할 수 있습니다.

2. `Soundex` 알고리즘은 단어의 음성학을 비교하여 유사도를 평가합니다.

   기본적으로 `Soundex` 알고리즘은 영어 단어의 발음을 기준으로 하며, 유사한 발음을 가진 단어들은 같은 코드를 가지게 됩니다. 따라서 이 알고리즘을 사용하면 발음이 비슷한 단어들을 찾아내는데 사용됩니다.

   <h3>알고리즘의 원리를 간단하게 정리하면</h3>

   1. 이름의 첫번째 글자를 저장하고, 첫번째 글자를 제외한 나머지 글자 중에서 a, e, h, i, o, u, w, y는 모두 제거합니다.

   2. 나머지 글자들에 대해서는 아래 표대로 번호를 부여합니다. 번호가 같은 글자들은 발음이 비슷하다고 판단되는 글자들입니다.

      ```
      b, f, p, v — 1
      c, g, j, k, q, s, x, z — 2
      d, t — 3
      l — 4
      m, n — 5
      r — 6
      ```

   3. 연속으로 같은 번호를 부여받으면 맨 앞에 하나만 남깁니다.

   4. 최종적으로 글자, 숫자, 숫자... 형태를 맞추기 위하여 코드의 길이가 4미만이면 0을 붙여 형태를 맞춥니다.

    <h3>"javascript"를 예시로 단계별로 어떤 과정을 거치는지 확인해보겠습니다</h3>

   1. 첫번째 단계에서 "javascript"도 첫글자 "j"를 시작코드로 받게 되고 남은 글자는 "vscrpt"가 됩니다.

   2. 두번째 단계에서 남은 글자들에 번호를 부여하면 코드는 "j122613"이 됩니다.

   3. 세번째 단계에서는 같은 번호가 연속인 경우 맨 앞의 번호만 남기면 코드는 "j12613"으로 완성됩니다.

   <p align="center">
     <img width="400px" alt="google-search-pattern" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/300b2277-8f71-444a-977b-85e8f1120a20" />
   </p>

   이런식으로 나온 코드 "j12613"은 "javascrpt", "javascriptt" 등과 같은 가능한 오타들의 코드와 일치합니다. 이렇게 같은 코드를 가진 단어들은 서로 유사한 단어임을 확인 할 수 있습니다.

### 2-3. 구현: 영단어 라이브러리와 트라이 자료구조로 올바른 검색어 추천

저희는 사용자의 검색어와 유사도가 높은 단어를 추천해 주기로 했고 이를 위해서는 검색어와 **비교할 단어들(올바른 단어들)** 이 필요했습니다.

저희는 크롤링 과정에서 검색어와 비교해줄 단어들을 수집했습니다. 크롤링하는 동안 모든 단어는 데이터베이스에 저장되었고, 사용자가 검색을 수행하면 유사도가 높은 단어를 찾아서 추천했습니다.

하지만 이 과정에서 두 가지 큰 문제가 발생했습니다.

1. **비교해줄 단어들안에 오타가 있는 단어들이 존재**, 크롤링을 통해 단어를 수집했기 때문에 일부 잘못된 단어가 포함되었습니다.
2. **검색에 많은 시간 소요**, 데이터베이스에서 수만 개의 단어를 검색하는 데 예상보다 많은 시간이 걸렸습니다.

이러한 문제를 해결하기 위해 다음과 같은 방법을 적용했습니다.

첫 번째, **영단어 라이브러리 사용**. 크롤링한 단어들을 사용하지 않고 정확한 단어로 채워진 라이브러리를 사용하여 정확도를 개선했습니다. 개발 관련 용어로 라이브러리에 포함되지 않은 단어들을 저희가 추가해주었습니다.

두 번째, **트라이(trie) 데이터 구조**를 활용하여 사용자의 검색어가 올바른지 빠르게 판단하였고 이러한 경우에만 맞춤법 검사 알고리즘을 적용했습니다. <br /> 트라이 구조는 문자열을 저장하고 탐색하는데 적합합니다. 문자열의 길이가 `n` 이라면 O(n)의 시간복잡도를 가집니다. 또한 트라이 구조를 사용하면 접두사를 공유하는 구조로 인해 중복되는 부분을 공유하여 공간 효율성에서 장점을 가집니다.

위 두 가지 방법을 통해 검색 시스템의 속도와 정확도를 모두 향상시킬 수 있었습니다.

<br />

# 🕹️ Advanced Features

## 1. 영상 속에 코드를 타이핑하지 않고 바로 가져다 쓸 수 없을까?

저희가 만든 검색 엔진은 개발과 프로그래밍 관련 영상들이라는 점에서 사용자들에게 어떤 기능이 있으면 좋을까를 고민해보았습니다. 팀원들 모두 영상을 보며 코드를 직접 타이핑했던 경험이 있었습니다. <br />
영상을 시청하다가 영상 속 코드를 손으로 타이핑하는 대신 텍스트로 복사해 주는 기능이 있으면 좋겠다라는 의견을 받아 해당 기능을 구현하기로 했습니다.

### 1-1. 유튜브 영상은 스크린 캡쳐를 할 수 없다

보통 브라우저 스크린을 캡처하기 위해서는 DOM 요소에 직접 접근하여 canvas를 사용합니다. <br />웹은 기본적으로 DOM 요소에 직접 접근하여 해당 요소의 현재 상태를 canvas에 그릴 수 있도록 합니다. 하지만 저희는 Iframe으로 **유튜브 영상**을 임베드하고 있기 때문에 외부 리소스 접근 권한 문제로 직접적으로 영상 DOM에 접근할 수 없었습니다.

### 1-2. Headless Browser를 이용하자

이에 생각해 낸 방법은 크롤러를 사용할 때처럼 puppeteer 라이브러리를 사용해 실제 유튜브 웹 페이지를 headless browser로 접근하여 스크린 캡처를 하는 것이었습니다.

유튜브 웹 페이지에서 영상은 Iframe이 아닌 Video 태그로 이루어져 있기 때문에 DOM에 접근이 가능하였고, 이를 활용해 사용자가 보는 비디오의 비디오 id, 현재 영상 재생 시점 두 가지 정보를 토대로 사용자가 보고 있는 영상 시점의 스크린을 캡처할 수 있었습니다.

유튜브 웹 사이트에 접근할 때 광고가 나오게 되면 스크린 캡쳐를 하는데 문제가 생깁니다. 이 문제는 puppeteer-adblocker 라이브러리를 사용하여 해결할 수 있었습니다.

### 1-3. 드래그 앤 드롭으로 원하는 코드만 추출 하기

사용자가 영상 속의 모든 글자, 코드를 원하지 않을 때가 많을 것입니다. 저희는 영상 속 특정 부분의 코드를 추출하기 위해서 드래그 앤 드롭 기능을 추가 했습니다.

비디오의 크기는 반응형으로 구현되어 화면의 크기에 따라 달라집니다. 이때 사용자가 드래그 드롭으로 선택한 부분이 headless browser에서 실제로 스크린을 캡쳐할 때와 같은 부분을 선택할 수 있어야 했습니다.

<p align="center">
  <img width="48%" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/69300f6e-4ef0-43e7-a95c-9a58fa9cd86f" alt="nih-video">
  <img width="48%" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/6f8e73cd-4f0f-4e51-8e15-e2d603b23e2f" alt="youtube-video">
  <figcaption align="center">needle in haystack 웹사이트(왼쪽) | 유튜브 웹사이트(오른쪽)</figcaption>
</p>

headless browser에서 실제로 캡쳐한 이미지의 크기와 사용자가 보는 비디오의 크기를 비교하여 비율을 구했습니다.
사용자가 드래그 드롭으로 선택한 영역도

<p align="center">
  <img width="600px" alt="screen-ratio" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/30fed10b-738f-4a65-9fdb-9878c7d23667">
</p>

사용자가 드래그 드롭으로 영상의 부분을 선택할 때의 비디오의 좌표와 비율을 이용해서 캡쳐하려는 영상에서 선택 범위를 비율에 맞게 구하여 해당 부분의 코드만 텍스트로 변환하였습니다.

영상 속 코드를 추출하기 위해서 이미지에서 텍스트를 인식하는 광학 문자 인식 기술인 OCR(Optical Character Recognition) 기술을 활용하였습니다. Node. JS 환경에서 가장 널리 사용되는 OCR 엔진 tesserect.js를 사용했습니다.

<p align="center">
  <img width="300px" alt="extract" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/139360841/19d22e2c-0c80-4819-9fd8-12d1252391b3">
</p>

### 1-4 구현: 추출한 코드를 코드 에디터로 편집

영상 속 코드를 텍스트로 변환하는 기능은 자바스크립트의 OCR엔진인 tesserect.js을 사용하여 구현했습니다.
이때 추가로 고민했던 점은 어떻게 자바스크립트 코드처럼 들여쓰기를 해줄 지 였습니다.

저희는 monaco-editor를 추가하여 서버에서 받은 코드 텍스트를 에디터에 넣어주고 자동으로 들여쓰기 및 개행이 된 코드를 사용자에게 제공해주는 방식으로 이를 해결했습니다.

코드 추출 기능을 사용하면 스크립트를 보여주던 화면이 자동으로 코드 에디터로 변경되고 추출된 코드가 에디터에 뜨면서 자동으로 사용자 클립보드에 복사가 됩니다.

<p align="center">
  <img width="600" alt="image" src="https://github.com/Team-Office360/NeedleInHaystack-client/assets/133403759/0c5be493-3334-49ea-a1e9-9706f824e0c3">
</p>

<br />

<!-- ## 2. 앱 애플리케이션 개발로 사용자 접근성 확장하기

문제상황: 사용자의 접근성을 높일 수 있는 방법은 없을까

### 2-1. Flutter 앱의 WebView를 활용하여 구현 -->

## ⏰ 프로젝트 타임라인

**1주차**

- 아이디어 수집, 선정
- POC 및 주제 관련 조사
- Git 작업 플로우 결정
- 기술 스택 결정
- ESLint, Prettier, Husky 설정
- 칸반 태스크 작성

**2주차**

- 리액트 및 Node.js/Express 환경 세팅
- 헤더 및 메인 페이지 구현
- 수동 크롤러 구현
- 검색 알고리즘 구현 (TF-IDF, BM25)

**3주차**

- 검색 결과 페이지 구현
- 영상 디테일 페이지 구현
- 추천 검색어 기능 구현
- 스펠링 자동 교정 기능 구현
- 관리자 페이지 추가 (관리자 페이지에서 크롤러 ON/OFF 기능 구현)
- 검색 알고리즘 고도화 (BM25F)

**4주차**

- 로그인 기능 및 로그인 시 검색 기록 저장 기능 구현
- AWS 람다 서비스를 활용한 자동 크롤러 구현
- 코드 추출 기능 구현
- 검색 알고리즘 고도화 (Page Rank)

**5주차**

- 플러터로 앱 구현

1. 메인 페이지: 플러터
2. 검색 결과, 디테일 페이지: Webview

## 📚 프로젝트 소감

프로젝트를 통해 검색 엔진을 조사해 보고 구현해 보았습니다.
늘 사용하지만, 잘 몰랐던 검색의 원리를 조사해 보면서 보이는 것보다 훨씬 더 복잡한 과정들이 있다는 것을 알게 되었습니다.
실제 사용되는 서비스에 비하면 부족한 점이 많겠지만 핵심 원리들을 찾아가며 하나하나 팀원들과 직접 만들어가는 과정에서 많은 것을 배웠다고 생각합니다.
